Prac1
A] Design a simple linear neural network model.
Code:
x = float(input("Entre the value of x"))
b = float(input("Entre the value of bias"))
w = float(input("Entre the value of weight"))
print("x=" ,x)
print("bias=" ,b)
print("weight=" ,w)
y = w*x + b
print(f'Value of y is {y}')
B] Calculate the output of neural net using both binary and bipolar sigmoidal function.
Code:
#B] Calculate the output of neural net using both binary and bipolar sigmoidal function.
import numpy as np
n = int(input("Enter the number of neurons: "))
w = []
x = []
#taking the values of input and their weight
for i in range (0, n):
    a = float(input("Enter the input: "))
    x.append(a)
    b = float(input("Enter the weight: "))
    w.append(b)
print("The given weights are: ")
print (w)
print("The given input are: ")
print(x)
y = 0.0
for i in range(0,n):
    y = y + (w[i]*x[i])
print("The net input y is")
print(y)

#Applying Binary Sigmoid fuction on the net input i.e y
binary = 1/(1+np.e**(-y))
print ("Binary output after applying sigmoid function:")
print(round(binary,3))

#Applying Bipolor Sigmoid fuction on the net input i.e y
bipolar = -1+(2/(1+(np.e**(-y))))
print("The output after applying Bipolar function:")
print(round(bipolar,3))

Prac2
A] Generate AND/NOT function using McCulloch-Pitts neural net.
Code:
threshold=0.5
x1=[]
x2=[]
w1 = 1 
w2 =-1
yt=[]
yout=[]
for i in range(4):
    a=int(input("Enter the input x1: "))
    x1.append(a)
    b=int(input("Enter the output x2: "))
    x2.append(b)
    c=int(input("Enter thr input yt: "))
    yt.append(c)

print("x1=",x1)
print("x2=",x2)
print("yt=",yt)
for  j in range(4):
    y=x1[j]*w1 + x2[j]*w2
    if(y>threshold):
        yout.append('1')
    else:
        yout.append('0')
print("yout: ", yout)
B] Generate XOR function using McCulloch-Pitts neural net.
Code:
theta = 1
def output(w,X):
    return [int(x1*w[0] + x2*w[1] >=1) for x1,x2 in X]
X = [(0,0),(0,1),(1,0),(1,1)]
print(f'X = {X}')
w1 = [1,-1]
Y1 = output(w1,X)
print(f'Y1 = {Y1}')
w2 = [-1,1]
Y2 = output(w2,X)
print(f'Y2 = {Y2}')
Y = list(zip(Y1,Y2))
print(f'Y = {Y}')
w3 = [1,1]
Z = output(w3,Y)
print(f'Z = {Z}')

Prac3
A] Write a program to implement Hebbâ€™s rule.
Code:
import numpy as np
#fisrt pattern
x1 = np.array([1,1,1,-1,1,-1,1,1,1])
#second pattern
x2 = np.array([1,1,1,1,-1,1,1,1,1])
#initialize bais value
b=0
#Define target
y=np.array([1,-1])
wold = np.zeros((9,))
wnew = np.zeros((9,))
wnew=wnew.astype(int)
bais=0
print("First input with target = 1")
for i in range(0,9):
    wold[i]=wold[i]+x1[i]*y[0]
wnew=wold
b=b+y[0]
print("Second input with target = -1")
for i in range(0,9):
    wnew[i]=wold[i]+x2[i]*y[1]
b=b+y[1]
print("new weight", wnew)
print("bias", b)
B] Write a program to implement Delta rule.
COde:
import numpy as np
import time
np.set_printoptions(precision = 2)
x = np.zeros((3,))
weights = np.zeros((3,))
desired = np.zeros((3,))
actual = np.zeros((3,))
for i in range(0,3):
    x[i] = float(input("Initial Inputs:"))

for i in range(0,3):
    weights[i] = float(input("Initial weights:"))

for i in range(0,3):
    desired[i] = float(input("Initial Desired:"))

a = float(input("Enter learning rate:"))
print("Actual",actual)
print("Desired",desired)

while True:
    if np.array_equal(desired,actual):
        break
    else:
        for i in range(0,3):
            weights[i] = weights[i] + a *(desired[i] - actual[i])
            actual = x*weights
            print("Weights:",weights)

prac4
A] Write a program for basic back propogation algorithm.
Code:
import numpy as np
X=np.array(([2,9],[1,5],[3,6]),dtype=float)
Y=np.array(([92],[86],[89]),dtype=float)
#scale units
X=X/np.amax(X,axis=0)
Y=Y/100;
class NN(object):
    def __init__(self):
        self.inputsize=2
        self.outputsize=1
        self.hiddensize=3
        self.W1=np.random.randn(self.inputsize,self.hiddensize)
        self.W2=np.random.randn(self.hiddensize,self.outputsize)
    def forward(self,X):
        self.z=np.dot(X,self.W1)
        self.z2=self.sigmoidal(self.z)
        self.z3=np.dot(self.z2,self.W2)
        op=self.sigmoidal(self.z3)
        return op;
    def sigmoidal(self,s):
        return 1/(1+np.exp(-s))
obj=NN()
op=obj.forward(X)
print("actual output\n"+str(op))
print("expected output\n"+str(Y))
B] Write a program for Error BackPropagation Algorithm.
Code:
import numpy as np
X=np.array(([2,9],[1,5],[3,6]),dtype=float)
Y=np.array(([92],[86],[89]),dtype=float)
X=X/np.amax(X,axis=0)
Y=Y/100;
class NN(object):
    def __init__(self):
        self.inputsize=2
        self.outputsize=1
        self.hiddensize=3
        self.W1=np.random.randn(self.inputsize,self.hiddensize)
        self.W2=np.random.randn(self.hiddensize,self.outputsize)
    def forward(self,X):
        self.z=np.dot(X,self.W1)
        self.z2=self.sigmoidal(self.z)
        self.z3=np.dot(self.z2,self.W2)
        op=self.sigmoidal(self.z3)
        return op;
    def sigmoidal(self,s):
        return 1/(1+np.exp(-s))
    def sigmoidalprime(self,s):
        return s* (1-s)
    def backward(self,X,Y,o):
        self.o_error=Y-o
        self.o_delta=self.o_error * self.sigmoidalprime(o)
        self.z2_error=self.o_delta.dot(self.W2.T)
        self.z2_delta=self.z2_error * self.sigmoidalprime(self.z2)
        self.W1 = self.W1 + X.T.dot(self.z2_delta)
        self.W2= self.W2+ self.z2.T.dot(self.o_delta)

    def train(self,X,Y):
        o=self.forward(X)
        self.backward(X,Y,o)
obj=NN()
for i in range(2000):
    print("input"+str(X))
    print("Actual output"+str(Y))
    print("Predicted output"+str(obj.forward(X)))
    print("loss"+str(np.mean(np.square(Y-obj.forward(X)))))
    obj.train(X,Y)

prac5
A] Write a program for Hopfield Network.
Code:
import numpy as np
def compute_next_state(state,weight):
    next_state = np.where(weight @ state>= 0, +1, -1)
    return next_state
def compute_final_state(initial_state,weight,max_iter=1000):
    previous_state = initial_state
    next_state =compute_next_state(previous_state,weight)
    is_stable = np.all(previous_state == next_state)
    n_iter = 0

    while(not is_stable) and (n_iter <= max_iter):
        previous_state = next_state
        next_state = compute_next_state(previous_state,weight)
        is_stable = np.all(previous_state==next_state)
        n_iter +=1
    return previous_state, is_stable,n_iter
initial_state = np.array([+1,-1,-1,-1])
weight = np.array([
        [0, -1, -1, +1],
        [-1, 0, +1, -1],
        [-1,+1,  0, -1],
        [+1,-1, -1,  0]])     
final_state, is_stable, n_iter = compute_final_state(initial_state,weight)
print("Final state",final_state)
print("is_Stable",is_stable)  
B]Write a program for Radial Basis function .
COde:(R program)

Prac6
A] Kohonen Self organizing map.
Code:
import minisom
from minisom import MiniSom 
import matplotlib.pyplot as plt

data = [[ 0.80, 0.55, 0.22, 0.03],
 [ 0.82, 0.50, 0.23, 0.03],
 [ 0.80, 0.54, 0.22, 0.03],
 [ 0.80, 0.53, 0.26, 0.03],
 [ 0.79, 0.56, 0.22, 0.03],
 [ 0.75, 0.60, 0.25, 0.03],
 [ 0.77, 0.59, 0.22, 0.03]] 
som = MiniSom(8, 8, 4, sigma=0.3, learning_rate=0.5) # initialization of 6x6 SOM
som.train_random(data, 100) # trains the SOM with 100 iterations
plt.imshow(som.distance_map())
B] Adaptive Resonance Theory.
COde:
from __future__ import print_function
from __future__ import division
import numpy as np

class ART:
    def __init__(self, n=5, m=10, rho=.5):
        
        self.F1 = np.ones(n)
        self.F2 = np.ones(m)
        self.Wf = np.random.random((m,n))
        self.Wb = np.random.random((n,m))
        self.rho = rho
        self.active = 0

    def learn(self, X):
        self.F2[...] = np.dot(self.Wf, X)
        I = np.argsort(self.F2[:self.active].ravel())[::-1]

        for i in I:
            
            d = (self.Wb[:,i]*X).sum()/X.sum()
            if d >= self.rho:
                
                self.Wb[:,i] *= X
                self.Wf[i,:] = self.Wb[:,i]/(0.5+self.Wb[:,i].sum())
                return self.Wb[:,i], i

        if self.active < self.F2.size:
            i = self.active
            self.Wb[:,i] *= X
            self.Wf[i,:] = self.Wb[:,i]/(0.5+self.Wb[:,i].sum())
            self.active += 1
            return self.Wb[:,i], i

        return None,None
if __name__ == '__main__':

    np.random.seed(1)
    network = ART( 5, 10, rho=0.5)


    data = ["   O ",
            "  O O",
            "    O",
            "  O O",
            "    O",
            "  O O",
            "    O",
            " OO O",
            " OO  ",
            " OO O",
            " OO  ",
            "OOO  ",
            "OO   ",
            "O    ",
            "OO   ",
            "OOO  ",
            "OOOO ",
            "OOOOO",
            "O    ",
            " O   ",
            "  O  ",
            "   O ",
            "    O",
            "  O O",
            " OO O",
            " OO  ",
            "OOO  ",
            "OO   ",
            "OOOO ",
            "OOOOO"]
    X = np.zeros(len(data[0]))
    for i in range(len(data)):
        for j in range(len(data[i])):
            X[j] = (data[i][j] == 'O')
        Z, k = network.learn(X)
        print("|%s|"%data[i],"-> class", k)

Prac7
A] Write a program for Linear Separation.
Code:
import numpy as np
import matplotlib.pyplot as plt 
def create_distance_function(a, b, c):
    """ 0 = ax + by + c """
    def distance(x, y):
        """ returns tuple (d, pos)
            d is the distance
            If pos == -1 point is below the line, 
            0 on the line and +1 if above the line
        """
        nom = a * x + b * y + c
        if nom == 0:
            pos = 0
        elif (nom<0 and b<0) or (nom>0 and b>0):
            pos = -1
        else:
            pos = 1
        return (np.absolute(nom) / np.sqrt( a ** 2 + b ** 2), pos)
    return distance
points = [ (3.5, 1.8), (1.1, 3.9) ]
fig, ax = plt.subplots()
ax.set_xlabel("Sweet")
ax.set_ylabel("Sour")
ax.set_xlim([-1, 6])
ax.set_ylim([-1, 8])
X = np.arange(-0.5, 5, 0.1)
colors = ["r", ""] # for the samples
size = 10
for (index, (x, y)) in enumerate(points):
    if index== 0:
        ax.plot(x, y, "o", 
                color="darkorange", 
                markersize=size)
    else:
        ax.plot(x, y, "oy", 
                markersize=size)
step = 0.05
for x in np.arange(0, 1+step, step):
    slope = np.tan(np.arccos(x))
    dist4line1 = create_distance_function(slope, -1, 0)
    #print("x: ", x, "slope: ", slope)
    Y = slope * X
    
    results = []
    for point in points:
        results.append(dist4line1(*point))
         #print(slope, results)
    if (results[0][1] != results[1][1]):
        ax.plot(X, Y, "g-")
    else:
        ax.plot(X, Y, "r-")    
plt.show()
B] Write a program for Hopfield Network model for associative memory.
Code:
from neurodynex.hopfield_network import network, pattern_tools, plot_tools
import matplotlib.pyplot as plt
pattern_size = 5
# create an instance of the class HopfieldNetwork
hopfield_net = network.HopfieldNetwork(nr_neurons= pattern_size**2)
# instantiate a pattern factory
factory = pattern_tools.PatternFactory(pattern_size, pattern_size)

# create a checkerboard pattern and add it to the pattern list
checkerboard = factory.create_checkerboard()
pattern_list = [checkerboard]

# add random patterns to the list
pattern_list.extend(factory.create_random_pattern_list(nr_patterns=3, on_probability=0.5))
plot_tools.plot_pattern_list(pattern_list)
# how similar are the random patterns and the checkerboard? Check the overlaps
overlap_matrix = pattern_tools.compute_overlap_matrix(pattern_list)
plot_tools.plot_overlap_matrix(overlap_matrix)
# let the hopfield network "learn" the patterns. Note: they are not stored
# explicitly but only network weights are updated !
hopfield_net.store_patterns(pattern_list)
# create a noisy version of a pattern and use that to initialize the network
noisy_init_state = pattern_tools.flip_n(checkerboard, nr_of_flips=4)
hopfield_net.set_state_from_pattern(noisy_init_state)
# from this initial state, let the network dynamics evolve.
states = hopfield_net.run_with_monitoring(nr_steps=4)
# each network state is a vector. reshape it to the same shape used to create the patterns.
states_as_patterns = factory.reshape_patterns(states)
# plot the states of the network
plot_tools.plot_state_sequence_and_overlap(states_as_patterns, pattern_list, 
reference_idx=0, suptitle="Network dynamics")

Prac8
A] Membership and Identity Operators | in, not in,
Code:
import skfuzzy as fuzz
import numpy as np
import matplotlib.pyplot as plt
# Creating universe of discourse
X = np.arange(150,191,1)  # Height of a person
# Create fuzzy set
short = fuzz.trimf(X,[150,160,170])
medium = fuzz.trimf(X,[160,170,180])
tall = fuzz.trimf(X,[170,180,190])
# Visualize the membership function
plt.figure(figsize=(10,5))
# Plot Short Height
plt.plot(X,short,'r',label='Short',linewidth=2)
# PLot Medium Height
plt.plot(X,medium,'b',label='Medium',linewidth=3)
# Plot Tall Height
plt.plot(X,tall,'g',label='Tall',linewidth=4)
# Test Data
test_heights = [153,165,172,178,185]
# Calculate the membership values for the given height in the short,medium and tall fuzzy sets
for height in test_heights:
    membership_tall = fuzz.interp_membership(X,tall,height)
    membership_medium = fuzz.interp_membership(X,medium,height)
    membership_short = fuzz.interp_membership(X,short,height)
    print()
# Highlight the membership values of test heights
for height in test_heights:
    plt.axvline(x=height,color='g',linestyle='-',linewidth=2)
plt.xlabel('Height')
plt.ylabel('Membership')
plt.legend()
plt.title('Fuzzy Set Membership')
plt.show()

B] Membership and Identity Operators is, is not
Code:
import skfuzzy as fuzz
import numpy as np
import matplotlib.pyplot as plt
# Create a universe of Discourse
x = np.arange(0,11,1)
# Create a fuzzy sets
small = fuzz.trimf(x,[0,0,5])
medium = fuzz.trimf(x,[0,5,10])
# Define test values
test_value1 = 2  # A value that 'is' in the small set
test_value2 = 8  # A value that 'is not' in the small set
# Calculate membership values for the test values
membership_small1 = fuzz.interp_membership(x,small,test_value1)
membership_medium1 = fuzz.interp_membership(x,medium,test_value1)
membership_small2 = fuzz.interp_membership(x,small,test_value2)
membership_medium2 = fuzz.interp_membership(x,medium,test_value2)
# Check if a value 'is' or 'is not' in the fuzzy sets
is_in_small1 = membership_small1>0.5
is_in_small2 = membership_small2>0.5
is_in_medium1 = membership_medium1>0.5
is_in_medium2 = membership_medium2>0.5
# Create plots to visualise membership values
plt.figure(figsize=(10,6))
plt.subplot(121)
plt.plot(x,small,'b',linewidth=2,label='Small')
plt.fill_between(x,0,small,alpha=0.2)
plt.plot([test_value1,test_value1],[0,membership_small1],'r',linestyle='--',linewidth=2)
plt.text(3,0.8,f'Membership={membership_small1}',fontsize=12)
plt.title('Membership in "Small" Set')
plt.xlabel('x')
plt.ylabel('Membership')
plt.subplot(122)
plt.plot(x,medium,'b',linewidth=2,label='Medium')
plt.fill_between(x,0,medium,alpha=0.2)
plt.plot([test_value1,test_value1],[0,membership_medium1],'r',linestyle='--',linewidth=2)
plt.text(3,0.8,f'Membership={membership_medium1}',fontsize=12)
plt.title('Membership in "Medium" Set')
plt.xlabel('x')
plt.ylabel('Membership')
# Display the plots
plt.tight_layout()
plt.show()
# Print the results
print(f'Test Value 1: {test_value1}')
print(f'Membership in small: {membership_small1}')
print(f'Membership in medium: {membership_medium1}')
print(f'Is in Small: {is_in_small1}\n')
print(f'Test Value 2: {test_value2}')
print(f'Membership in small: {membership_small2}')
print(f'Membership in medium: {membership_medium2}')
print(f'Is in Small: {is_in_small2}\n')

Prac9
A] Find ratios using fuzzy logic.
Code:
import numpy as np
import skfuzzy as fuzz
from skfuzzy import control as ctrl
# define linguistic variables and their ranges
similarity_ratio = ctrl.Consequent(np.arange(0,101,1),'Similarity Ratio')
char_similarity = ctrl.Antecedent(np.arange(0,101,1),'Character Similarity')
# define linguistic terms and their membership functions
similarity_ratio['low'] = fuzz.trimf(similarity_ratio.universe,[0,0,50])
similarity_ratio['medium'] = fuzz.trimf(similarity_ratio.universe,[0,50,100])
similarity_ratio['high'] = fuzz.trimf(similarity_ratio.universe,[50,100,100])
char_similarity['low'] = fuzz.trimf(char_similarity.universe,[0,0,50])
char_similarity['medium'] = fuzz.trimf(char_similarity.universe,[0,50,100])
char_similarity['high'] = fuzz.trimf(char_similarity.universe,[50,50,100])
# define fuzzy rules
rule1 = ctrl.Rule(char_similarity['low'],similarity_ratio['low'])
rule2 = ctrl.Rule(char_similarity['medium'],similarity_ratio['medium'])
rule3 = ctrl.Rule(char_similarity['high'],similarity_ratio['high'])
# create a control system
similarity_ctrl = ctrl.ControlSystem([rule1,rule2,rule3])
# create a simulation
similarity_sim = ctrl.ControlSystemSimulation(similarity_ctrl)
# function to compute character similarity
def calculate_char_similarity(str1,str2):
    common_chars = set(str1.lower()) & set(str2.lower())
    return len(common_chars)/max(len(str1),len(str2))*100
# Provide input values for the two strings
str_A = "Gunner William Kline"
str_B = "Kline, Gunner William"
# calculate character similarity
char_similarity_value = calculate_char_similarity(str_A,str_B)
# calculate similarity ratio using fuzzy logic
similarity_sim.input['Character Similarity'] = char_similarity_value
similarity_sim.compute()
# get the computed similarity ratio
similarity_ratio_value = similarity_sim.output['Similarity Ratio']
# print similarity ratio
print('Fuzzy Similarity Ratio:',similarity_ratio_value)
# visualize the membership functions and the input
char_similarity.view()
similarity_ratio.view()
char_similarity.view(similarity_sim)
similarity_ratio.view(similarity_sim)

B] Solve Tipping problem using fuzzy logic.
Code:
import numpy as np
import skfuzzy as fuzz
from skfuzzy import control as ctrl
import matplotlib.pyplot as plt
# define fuzzy variables and their ranges
service_quality = ctrl.Antecedent(np.arange(0,11,1),'Service Quality')
food_quality = ctrl.Antecedent(np.arange(0,11,1),'Food Quality')
tip = ctrl.Consequent(np.arange(0,26,1),'Tip Amount')
# define fuzzy sets and their membership function
service_quality.automf(3)
food_quality.automf(3)
tip.automf(3)
# define fuzzy rules
rule1 = ctrl.Rule(service_quality['poor'] | food_quality['poor'], tip['poor'])
rule2 = ctrl.Rule(service_quality['average'], tip['average'])
rule3 = ctrl.Rule(service_quality['good'] | food_quality['good'], tip['good'])
# create a control system
tipping_control = ctrl.ControlSystem([rule1,rule2,rule3])
# create a simulation
tipping = ctrl.ControlSystemSimulation(tipping_control)
# provide input values
tipping.input['Service Quality'] = 8
tipping.input['Food Quality'] = 6
# compute tip
tipping.compute()
# visualize membership function
service_quality.view()
food_quality.view()
tip.view()
# printing the result
print('Recommended tipping amout: ',tipping.output['Tip Amount'], '%')
plt.show()

Prac10
A] Implementation of Simple genetic algorithm
Code:
import random
import matplotlib.pyplot as plt

# Genetic algorithm parameters
population_size = 100
mutation_rate = 0.01
generations = 100

# function to generate a random binary string
def generate_individual(length):
    return [random.randint(0, 1) for _ in range(length)]

# function to calculate the fitness of an individual (number of 1s in the binary string)
def calculate_fitness(individual):
    return sum(individual)

# function to select individuals for the next generation (roulette wheel selection)
def select(population, fitness_scores):
    total_fitness = sum(fitness_scores)
    normalized_fitness = [score / total_fitness for score in fitness_scores]
    selected = random.choices(population, weights=normalized_fitness, k=2)
    return selected

# function to perform one point crossover
def crossover(parent1, parent2):
    crossover_point = random.randint(1, len(parent1) - 1)
    child1 = parent1[:crossover_point] + parent2[crossover_point:]
    child2 = parent2[:crossover_point] + parent1[crossover_point:]
    return child1, child2

# function to perform mutation
def mutate(individual, rate):
    mutated = [bit if random.random() > rate else 1 - bit for bit in individual]
    return mutated

# Main Genetic Algorithm
def genetic_algorithm():
    # initialize a random population
    population = [generate_individual(20) for _ in range(population_size)]
    fitness_history = []

    for generation in range(generations):
        # calculate fitness for each individual
        fitness_scores = [calculate_fitness(individual) for individual in population]
        fitness_history.append(max(fitness_scores))

        # select parents for the next generation
        parents = [select(population, fitness_scores) for _ in range(population_size // 2)]

        # create the next generation through crossover and mutation
        next_generation = []
        for parent1, parent2 in parents:
            child1, child2 = crossover(parent1, parent2)
            child1 = mutate(child1, mutation_rate)
            child2 = mutate(child2, mutation_rate)
            next_generation.extend([child1, child2])

        population = next_generation

        # find and print the best individual in this generation
        best_individual = max(population, key=calculate_fitness)
        print(f'Generation {generation + 1}: {best_individual} - Fitness: {calculate_fitness(best_individual)}')

    # Plot fitness history
    plt.figure()
    plt.plot(range(generations), fitness_history)
    plt.xlabel('Generations')
    plt.ylabel('Best Fitness')
    plt.title('Genetic Algorithm Progress')
    plt.show()

# Run the genetic algorithm
genetic_algorithm()

B] Create two classes: City and Fitness using Genetic algorithm
Code:
import random
import matplotlib.pyplot as plt

# Genetic Algorithm Parameters
Population_size = 20
Genes = ['0', '1']
Target_city = '1100110110'
Mutation_rate = 0.1

# define a fitness function
def calculate_fitness(chromosome):
    fitness = sum(1 for a, b in zip(chromosome, Target_city) if a == b)
    return fitness

# create initial population
def create_population(population_size, genes, target_city):
    population = []
    for _ in range(population_size):
        chromosome = ''.join(random.choice(genes) for _ in range(len(target_city)))
        population.append(chromosome)
    return population

# Apply mutation to the population
def mutate_population(population, mutation_rate, genes):
    for i in range(len(population)):
        if random.random() < mutation_rate:
            idx = random.randint(0, len(population[i])-1)
            population[i] = population[i][:idx] + random.choice(genes) + population[i][idx:]
    return population

# Perform selection using tournament selection
def tournament_selection(population, fitness_values, k=3):
    selected = []
    for _ in range(len(population)):
        participants = random.sample(list(enumerate(fitness_values)), k)
        winner = max(participants, key=lambda x: x[1])[0]
        selected.append(population[winner])
    return selected

# Main genetic algorithm loop
def genetic_algorithm():
    population = create_population(Population_size, Genes, Target_city)
    best_fitness = []
    for generation in range(100):
        fitness_values = [calculate_fitness(chromosome) for chromosome in population]

        # Select the best fit individuals for reproduction
        selected_population = tournament_selection(population, fitness_values)

        # Apply Crossover
        next_generation = []
        for i in range(0, Population_size, 2):
            crossover_point = random.randint(1, len(Target_city)-1)
            child1 = selected_population[i][:crossover_point] + selected_population[i+1][crossover_point:]
            child2 = selected_population[i+1][:crossover_point] + selected_population[i][crossover_point:]
            next_generation.extend([child1, child2])

        # Mutate the population
        mutated_population = mutate_population(next_generation, Mutation_rate, Genes)
        population = mutated_population

        best_fit = max(list(zip(population, fitness_values)), key=lambda x: x[1])
        best_fitness.append(best_fit[1])

        print(f'Generation {generation + 1}: Best Fit - {best_fit[0]} with Fitness {best_fit[1]}')

    # Plotting the fitness improvement over generations
    plt.plot(best_fitness)
    plt.title('Fitness Improvement Over Generations')
    plt.xlabel('Generation')
    plt.ylabel('Fitness')
    plt.show()

# Run the genetic algorithm
genetic_algorithm()
